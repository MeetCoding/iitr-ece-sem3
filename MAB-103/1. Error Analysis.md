**Exact and Approximate Numbers:**  
Exact numbers represent values known precisely (like integers or defined constants), while approximate numbers carry uncertainty due to measurement or computation limitations.
**Rounding of Numbers:**  
Rounding reduces numbers to a fixed number of significant digits to fit storage or simplify calculations. It introduces **round-off errors**, which are differences between the true value and the rounded number.
**Significant Digits and Correct Digits:**
- _Significant digits_ are those digits in a number that carry meaningful information about its precision.
- _Correct digits_ refer to the number of digits in which an approximation matches the true value.
**Types of Errors in Computation:**
- **Absolute error:** The difference between the approximate value and the exact value.
- **Relative error:** The absolute error divided by the true value, expressing error magnitude relative to the size of the quantity.
- **Truncation error:** Error from approximating a mathematical procedure (e.g., stopping a series expansion early).
- **Round-off error:** Error caused by rounding numbers in finite-precision computations.
**Propagation of Error:**  Errors in input data or intermediate computations **propagate** through calculations, potentially amplifying the uncertainty in the result. For example, when subtracting two nearly equal numbers, the relative error can become very large, a phenomenon called **loss of significance**. Error propagation can be estimated using partial derivatives of functions, combining the individual input errors to estimate the overall output error.
# Floating point representation
The IEEE 754 standard defines consistent rules for representing and processing floating-point numbers in computers. It ensures accurate, portable, and predictable results across different systems by standardizing number formats, exception handling, and arithmetic behavior.

IEEE754 Floating-point representation is a cornerstone concept in both numerical analysis and computer arithmetic. It allows computers to approximate a wide range of real numbers using a standardized format.
$$float(x) = (-1)^s (0.d_1d_2\dots d_n)_\beta \beta^e$$
- $s$ represents the sign which can either be 0 or 1
- $0\le d_i< \beta$  are the significant digits after the decimal point
- $n$ represents the number of digits after the decimal point
- $\beta$ is the base of the number system, usually 10 or 2
- $e$ is the exponent which determines the scaling factor 
In normalized form, we adjust the number such that the first digit $d_1$ is non-zero for a non-zero number (in base 2 system, $\beta=2, d_1=1$). If $d_1=0$ then $x$ must be $0$ in normalized floating point representation.
$$(0.d_1d_2\dots d_n)_\beta=\dfrac{d_1}{\beta}+\dfrac{d_2}{\beta^2}\dots+\dfrac{d_n}{\beta^n}$$
*Example: for $x=-0.00023$, $\beta=10,e=-3,s=1,d_1=2,d_2=3$*
Every machine has a confined range for the exponent: $m<e<M$
- $e<m$ causes underflow, machine assumes the number as 0
- $e>M$ causes overflow, machine assumes the number undefined or throws some garbage value.
IEEE offers two types of floating point representations:
- Single precision representation stores the float in 4 bytes (32 bits) out of which 3 bytes store the matissa and sign (1 bit) and 1 byte is used to store the exponent $-126<e<127$
- Double precision representation stores the float in 53 bits with exponent $-1023<e<1023$
## Truncating Methods
Suppose you have an irrational number, let's take square root of two. 
$$\sqrt2=1.414213562...$$
Now we want to store this number in our machine. Let's say our machine has a power to store 5 digits, $n=5$. There are two ways to store the square root of two.
Firstly we write it in normalized floating point form.
$$\sqrt2=(-1)^0(0.1414213562...)\times10^{1}$$
We will now need to somehow make the number of digits equal or less than 5 as it is the capacity of our machine.
This can be done via either chopping the remaining digits or by rounding off.
$$float(\sqrt2)=(-1)^0(0.14142)\times10^{1}$$
Here, $s=0, n=5, \beta=10, e=1$
So, for a very long number (longer than the storing capacity of our machine) in the secant representation:
$$x=(-1)^s(.d_1d_2\dots d_nd_{n+1}...)\times\beta^e$$
We can write the floating representation in a machine with capacity n as:
$$float(x)=(-1)^s(.d_1d_2\dots \hat{d_n})\times\beta^e$$
In case of chopping, $\hat{d_n}=d_n$, but for rounding off, we follow some rules:
1. If $0\le d_{n+1}<\beta/2$ then  $\hat{d_n}=d_n$
2. If $\beta/2< d_{n+1}<\beta$ then $\hat{d_n}=d_n+1$
3. If $d_{n+1}=\beta/2$:
	1. If any $d_{n+i}>0$ for $i>1$, then $\hat{d_n}=d_n+1$
	2. If all $d_{n+i}=0$ for $i>1$ then we make $d_n$ even either by keeping it the same or adding $1$.
Consider the following examples for $n=3$
- $0.53244322\implies0.532$ (Floor)
- $0.53285234\implies 0.533$ (Ceil)
- $0.53252342\implies0.533$ (Ceil)
- $0.532500000\implies0.532$ (Make even, already even)
- $0.533500000\implies0.534$ (Make even, added 1)
## Rounding Errors
Let $x_A$ be the approximated value of $x$, usually for a machine, $x_A=float(x)$.

| Type             | Definition                       |
| ---------------- | -------------------------------- |
| Error            | $Er(x)=x-x_A$                    |
| Absolute Error   | $\|x-x_A\|$                      |
| Relative Error   | $\dfrac{\|x-x_A\|}{x}$           |
| Percentage Error | $\dfrac{\|x-x_A\|}{x}\times 100$ |
**$x_A$ approximates to $x$ to $r$ significant digits when:**
$$|x-x_A|\le 0.5\times\beta^{s-r+1}$$
Here, $s$ is the **largest possible number** such that $\beta^s<|x|$
*Example: let $x=7.65454$ and $x_A=7.65452$, then we know:*
$$|x-x_A|=0.00002=0.2\times10^{-4}\le0.5\times10^{-4}$$
*And we also know $10^{-1}<7.65454$, so $s=-1$. Note that $s<-1$ would also satisfy the relation, but we want the largest possible value of s*.
*Hence, $s-r+1=-4\implies r=4$, Therefore our approximation is accurate up to 4 significant values.*
